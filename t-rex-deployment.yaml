apiVersion: apps/v1
kind: Deployment
metadata:
  name: t-rex
  labels:
    app: t-rex  
spec:
  strategy:
    type: RollingUpdate
  # Replicas controls the number of instances of the Pod to maintain running at all times
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: t-rex
  template:
    metadata:
      labels:
        app.kubernetes.io/name: t-rex      
    spec:  
      containers:
        - name: t-rex
          #image: cryptojunkies/ethminer:latest-cuda9.2
          image: gershona/cuda-t-rex
          env:
          - name: WORKER_NAME
            value: "Gershon-t-rex"
          - name: WALLET
            value: {{ETH_WALLET}}
          - name: POOL
            value: {{POOL}}
          command: ["bash", "-c", "./t-rex -a ethash -o stratum+tcp://$POOL --user $WALLET --pass x --worker $HOSTNAME"]
# Mining Firo (Zcoin) https://xzc.2miners.com/help
##          command: ["bash", "-c", "./t-rex -a mtp -o stratum+tcp://xzc.2miners.com:8080 -u a25MXWYXhhDzbnj2KRp89mC5pKr6b8SZpq.rig_$HOSTNAME -p x"]

          ports:
            - name: api
              containerPort: 4067
              protocol: TCP


          resources:
##            requests:
#              cpu: 500m # The CPU unit is mili-cores. 500m is 0.5 cores
##              cpu: 250m # The CPU unit is mili-cores. 250m is 0.25 cores
##              memory: 64Mi
            limits:
            #  cpu: 2000m
              cpu: 1000m # 1000m is 1 core
              memory: 512Mi
              # GPUs can only be allocated as a limit, which both reserves and limits the number of GPUs the Pod will have access to
              # Making individual Pods resource light is advantageous for bin-packing. In the case of Ethminer, running a single GPU
              # per Pod would work well and often be preferred. 
              # In case we have 2 GPUs change nvidia.com/gpu: 2 possible gain more performance
              # nvidia.com/gpu: 1

      # Node affinity can be used to require / prefer the Pods to be scheduled on a node with a specific hardware type
      # No affinity allows scheduling on all hardware types that can fulfill the resource request.
      # In this example, without affinity, any NVIDIA GPU would be allowed to run the Pod.
      # Read more about affinity at: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
#      affinity:
#        nodeAffinity:
#          # This will REQUIRE the Pod to be run on a system with a multi purpose NVIDIA Pascal series GPU
#          requiredDuringSchedulingIgnoredDuringExecution:
#           nodeSelectorTerms:
#            - matchExpressions:
#              - key: gpu.nvidia.com/class
#                operator: In
#                values:
#                  - NV_Pascal
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: beta.kubernetes.io/instance-type
                operator: In
                values:
                - g3s.xlarge
                - g4dn.xlarge
                - p3.2xlarge
                - p3.8xlarge
                - p3.16xlarge
                - p3dn.24xlarge
                - p2.xlarge
                - p2.8xlarge
                - p2.16xlarge
        podAntiAffinity:
#          requiredDuringSchedulingIgnoredDuringExecution:
#          - labelSelector:
#              matchExpressions:
#              - key: workgroup
#                operator: In
#                values:
#               - g4dn-xlarge
#            topologyKey: "kubernetes.io/hostname" 
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values:
                    - t-rex
              topologyKey: "kubernetes.io/hostname"           
          # As mining doesn't require much CPU and very little network connectivity, we try to play nice and only schedule
          # the Pod on systems with Celeron family CPUs and 1G network connections. This is a preference, not a requirement.
          # If systems with Celeron CPUs and/or 1G ethernet are not available to fulfill the requested resources, the Pods
          # will be scheduled on higher end systems.
#          preferredDuringSchedulingIgnoredDuringExecution:
#            - weight: 10
#              preference:
#                matchExpressions:
#                - key: cpu.coreweave.cloud/family
#                  operator: In
#                  values:
#                    - celeron
#            - weight: 10
#              preference:
#                matchExpressions:
#                - key: ethernet.coreweave.cloud/speed
#                  operator: In
#                  values:
#                    - 1G

      restartPolicy: Always
