apiVersion: apps/v1
kind: Deployment
metadata:
  name: ethminer
  labels:
    app: ethminer  
spec:
  strategy:
    type: RollingUpdate
  # Replicas controls the number of instances of the Pod to maintain running at all times
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: ethminer
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ethminer
#### Shedule on GPU node ###        
    spec:
#      nodeSelector:
#        nvidia.com/gpu: "true"    
      containers:
        - name: ethminer
          image: ahtonen/docker-ethminer         
          env:
          - name: WORKER_NAME
            value: {{WORKER_NAME}}
          - name: ETH_WALLET
            value: {{ETH_WALLET}}
          resources:
            requests:
              cpu: 250m # The CPU unit is mili-cores. 250m is 0.25 cores
              memory: 64Mi
            limits:
              cpu: 1000m
              memory: 512Mi
              # GPUs can only be allocated as a limit, which both reserves and limits the number of GPUs the Pod will have access to
              # Making individual Pods resource light is advantageous for bin-packing. In the case of Ethminer, running a single GPU
              # per Pod would work well and often be preferred. In this example we use 2 GPUs per pod to showcase the flexibility
              # of GPU allocation.
              # nvidia.com/gpu: 1

      # Node affinity can be used to require / prefer the Pods to be scheduled on a node with a specific hardware type
      # No affinity allows scheduling on all hardware types that can fulfill the resource request.
      # In this example, without affinity, any NVIDIA GPU would be allowed to run the Pod.
      # Read more about affinity at: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
#      affinity:
#        nodeAffinity:
#          # This will REQUIRE the Pod to be run on a system with a multi purpose NVIDIA Pascal series GPU
#          requiredDuringSchedulingIgnoredDuringExecution:
#           nodeSelectorTerms:
#            - matchExpressions:
#              - key: gpu.nvidia.com/class
#                operator: In
#                values:
#                  - NV_Pascal
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: beta.kubernetes.io/instance-type
                operator: In
                values:
                - g3s.xlarge
                - g4dn.xlarge
                - p3.2xlarge
                - p3.8xlarge
                - p3.16xlarge
                - p3dn.24xlarge
                - p2.xlarge
                - p2.8xlarge
                - p2.16xlarge
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: workgroup
                operator: In
                values:
                - g3s-xlarge
            topologyKey: "kubernetes.io/hostname"                
          # As mining doesn't require much CPU and very little network connectivity, we try to play nice and only schedule
          # the Pod on systems with Celeron family CPUs and 1G network connections. This is a preference, not a requirement.
          # If systems with Celeron CPUs and/or 1G ethernet are not available to fulfill the requested resources, the Pods
          # will be scheduled on higher end systems.
#          preferredDuringSchedulingIgnoredDuringExecution:
#            - weight: 10
#              preference:
#                matchExpressions:
#                - key: cpu.coreweave.cloud/family
#                  operator: In
#                  values:
#                    - celeron
#            - weight: 10
#              preference:
#                matchExpressions:
#                - key: ethernet.coreweave.cloud/speed
#                  operator: In
#                  values:
#                    - 1G

      restartPolicy: Always
